###################################################################################
# This script contains the code to reproduce the analyses used to make Figures 1a, S1 and S2, and Table S1 of
# Stockey et al. - Decreasing Phanerozoic extinction intensity as a consequence of Earth surface oxygenation and metazoan ecophysiology
# Please note that the majority of this script is not original code, but code from 
# Kocsis et al. 2019 (Methods in Ecology and Evolution) - The r package divDyn for quantifying diversity dynamics using fossil sampling data
# For the vast majority of the analyses below, Kocsis et al. should be considered the primary reference (and is therefore extensively cited in Stockey et al.)
# and code has been directly pasted from the R Markdown file associated with that study. 
# Extensive documentation of the DivDyn R package and those analyses can be found here - https://github.com/divDyn/ddPhanero
# Additions made to perform the extended analyses in Stockey et al. are flagged using "#RGS addin" or simply "#RGS:"
###################################################################################

library(divDyn)
library(dplyr)
library(fANCOVA)
library(ggplot2)
#library(deeptime) <-  do not call deeptime until end of script because need to use two different "stages"!
detach("package:deeptime", unload = TRUE)
library(reshape2)
library(dplyr)

# set working directory 
setwd()

no.sp.vect <- c(NA, 1, 2, 3, 4)
filter.vec <- c("N", "Y", "Y", "Y", "Y")

extDatTen.Sum <- as.numeric()
extDatStg.Sum <- as.numeric()

extModel.Ten.Sum <- as.numeric()
extModel.Stg.Sum <- as.numeric()

statsBin.sum <- as.numeric()
statsStage.sum <- as.numeric()

# RGS addin
# set up a loop to go through scenarios of all taxa, then genera with 1, 2, 3 and 4+ species

for(no.sp in no.sp.vect){

filter_by_sp_per_genus <- filter.vec[match(no.sp, no.sp.vect)]

# RGS addin
# Load data download from PBDB - this download exactly replicates the one used in Kocsis et al. but was downloaded on June 22 2021 and therefore contains occurrences added since 2018
load(file="allData_2021-06-22.RData")

# omission of occurrences that were not identified to the level of genus
dat <- dat[dat$accepted_rank %in% c("genus", "species"), ]
dat <- dat[dat$genus!="", ]

# Omitting these poorly identified fossils decreases the sample size to 
nrow(dat) 

if(filter_by_sp_per_genus == "Y"){
# RGS addin - how many species per genus?
sp_p_genus <- dat %>%
  filter(accepted_rank == "species")  %>%
  group_by(genus, accepted_name)  %>%
  tally()  %>%
  group_by(genus)  %>%
  tally()

# RGS addin - Which genera have the number of species we are looking for?

if(no.sp < 4){
  genera_w_x_sp <- sp_p_genus %>%
    filter(n == no.sp)
}else{
  genera_w_x_sp <- sp_p_genus %>%
    filter(n >= no.sp)
}

sp_to_include <-  dat$genus %in% genera_w_x_sp$genus

# RGS addin - Filter to only include those genera
dat <- dat[sp_to_include, ]
}

# The filtering process continues with the selection of phyla that have or had recorded marine species. Phyla that have considerable terrestrial and marine records (chordates and arthropods) are included based on lower taxonomic ranks.
# sampled phyla
# levels(factors(dat$phylum))
# A. phyla
marineNoPlant <- c("",
                   "Agmata",
                   "Annelida",
                   "Bilateralomorpha",
                   "Brachiopoda",
                   "Bryozoa",
                   "Calcispongea",
                   "Chaetognatha",
                   "Cnidaria",
                   "Ctenophora",
                   "Echinodermata",
                   "Entoprocta",
                   "Foraminifera",
                   "Hemichordata",
                   "Hyolitha",
                   "Mollusca",
                   "Nematoda",
                   "Nematomorpha",
                   "Nemertina",
                   "Onychophora",
                   "Petalonamae",
                   "Phoronida",
                   "Platyhelminthes",
                   "Porifera",
                   "Problematica",
                   "Rhizopodea",
                   "Rotifera",
                   "Sarcomastigophora",
                   "Sipuncula",
                   "Uncertain",
                   "Vetulicolia",
                   ""
)

# Then a logical vector was defined for each row, suggesting whether the phylum of that occurrence is present or not in the above defined set ``marineNoPlant``.

# logical vector of rows indicating these
bByPhyla <- dat$phylum %in% marineNoPlant

# The rest of the data were saved for further filtering.  Classes that are likely to be marine animals were extracted from this subset.
# noNeed <- dat[!bByPhyla,]
#B. classes
#levels(factor(noNeed$class))
needClass <- c(
  "Acanthodii",
  "Actinopteri",
  "Actinopterygii",
  "Agnatha",
  "Cephalaspidomorphi",
  "Chondrichthyes",
  "Cladistia",
  "Coelacanthimorpha",
  "Conodonta",
  "Galeaspida",
  "Myxini",
  "Osteichthyes",
  "Petromyzontida",
  "Plagiostomi",
  "Pteraspidomorphi",
  "Artiopoda",
  "Branchiopoda",
  "Cephalocarida",
  "Copepoda",
  "Malacostraca",
  "Maxillopoda",
  "Megacheira",
  "Merostomoidea",
  "Ostracoda",
  "Paratrilobita",
  "Pycnogonida",
  "Remipedia",
  "Thylacocephala",
  "Trilobita",
  "Xiphosura"
)

# logical vector of rows indicating occurrences
bNeedClass <- dat$class %in% needClass

# The mammalian orders Sirenia and Cetacea were also included, as well as pinniped families from the order Carnivora.
#C. mammals
# mammals <- dat[dat$class=="Mammalia", ]
# levels(factor(mammals$order))
needMammalOrd <- c("Cetacea", "Sirenia")
bMammalOrder <- dat$order %in% needMammalOrd

# the carnivores
# carnivores <- dat[dat$order=="Carnivora", ]
# levels(factor(carnivores$family))
needFam <- c("Otariidae", "Phocidae", "Desmatophocidae")
bNeedMamFam <- dat$family %in% needFam

# Some reptile orders were also included:

# D. Reptiles
# reptiles <- dat[dat$class=="Reptilia", ]
# levels(factor(reptiles$order))

needReptOrd<-c(
  "Eosauropterygia",
  "Hupehsuchia",
  "Ichthyosauria",
  "Placodontia",
  "Sauropterygia",
  "Thalattosauria"
)
# the logical vector for the total data
bRept <- dat$order %in% needReptOrd	

#Families of sea turtles are also added to the analyzed set.

# E. Sea turtles
# turtles <- dat[dat$order=="Testudines", ]
# levels(factor(turtles$family))

needTurtleFam <- c(
  "Cheloniidae",
  "Protostegidae",
  "Dermochelyidae",
  "Dermochelyoidae",
  "Toxochelyidae",
  "Pancheloniidae"
)
bTurtle <- dat$family%in%needTurtleFam

# RGS addin: remove mammals, marine reptiles and turtles because all groups have endothermic (or perportedly endothermic) members, 
# and none obtain oxygen directly from the water.
dat <- dat[bByPhyla | bNeedClass, ] #  | bMammalOrder | bNeedMamFam | bRept | bTurtle, ]

# The entire procedure decreases the number of occurrences to...?
# the number of rows after taxonomic filtering
nrow(dat)

# After the filtering by taxonomy was finished, we can make sure that potential homonymies will not affect the results by combining the class names and genus names to create individual entries:
# resolve the potential homonymy problem
dat$clgen <- paste(dat$class, dat$genus)

## Filtering by sedimentary environment

# Some of the taxa above contain freshwater and/or terrestrial groups. These are expected to occur in such sedimentary environments, which is recorded in the ``environment`` field. We can list the sampled environments with this simple line of code.
# filter by environment
levels(factor((dat$environment)))

# The following environments are expected to contain terrestrial taxa.
omitEnv <- c(
  "\"floodplain\"", "alluvial fan", "cave", "\"channel\"", "channel lag" , 
  "coarse channel fill", "crater lake", "crevasse splay", "dry floodplain", 
  "delta plain", "dune", "eolian indet.", "fine channel fill", "fissure fill", 
  "fluvial indet.", "fluvial-lacustrine indet.", "fluvial-deltaic indet.", 
  "glacial", "interdune", "karst indet.", "lacustrine - large", 
  "lacustrine - small", "lacustrine delta front", "lacustrine delta plain", 
  "lacustrine deltaic indet.", "lacustrine indet.", 
  "lacustrine interdistributary bay", "lacustrine prodelta", "levee", "loess", 
  "mire/swamp", "pond", "sinkhole", "spring", "tar", "terrestrial indet.", 
  "wet floodplain")

# We can omit occurrences with these entries with a similar command as above.

# omit the occurrences
dat <- dat[!dat$environment%in%omitEnv, ]

# Although this filtering is not perfect, we believe that it is adequate for answering large-scale questions. The remaining non-marine and non-animal occurrences are unlikely to influence the results. After this filtering step 

nrow(dat)

# occurrences remain in the dataset. 
# Collections that came from unlithified sediments can yield fossils with unusually good preservation. As these are more frequent in younger sites and occur heterogeneously, sampling bias can be reduced by omitting such collections from the data.

dat <- dat[dat$lithification1!="unlithified", ]

# This last step leaves 
nrow(dat)
# occurrences for the analyses.

# For additional calculations, the environmental and lithological information can be categorized to either ``"reef"`` or ``"non-reef"`` (environment type),``"shallow"`` or ``"deep"`` (bathymetry), ``"siliciclastic"`` or ``"carbonate"`` (substrate type) and ``"coarse"`` or ``"fine"`` (grain size) entries. The ``keys`` object contains the information in a relevant form, which can be used by a grouping function ``categorize()`` that replaces groups of entries in a vector with single group names (see ``?categorize``).  
data(keys)
# using the others will be included in an appendix to this vignette
names(keys)

# Then the grouping can be applied with:
# siliciclastic or carbonate?
dat$lith <- categorize(dat$lithology1,keys$lith)

# batyhmetry
dat$bath <- categorize(dat$environment,keys$bath)

# grain size
dat$gra <- categorize(dat$lithology1,keys$grain) 

# reef or not?
dat$reef <- categorize(dat$environment, keys$reef) 
dat$reef[dat$lith=="clastic" & dat$environment=="marine indet."] <- "non-reef" 


# Although these variables will not be used in the following analyses, they are useful for answering many questions regarding habitat preferences, macroevolutionary differences between environments and extinction selectivity. 

## 3.3. Stratigraphic binning

# To use the occurrences in the ``divDyn`` package in an efficient way, the entries must be assigned to a discrete time scale. We included two of these time-scales in the package: the widely-used 10 myr time scale of the Paleobiology Database, and another one based on the stratigraphic stages of Ogg et al. (2016). These can loaded with the ``data()`` function.

# 10 million year timescale
data(tens)
# stage-level timescale
data(stages)


#You can learn more about these time scales if you type in ``?tens`` or ``?stages``. The first time scale (``tens``) has 49 entries identifying roughly 10-million year bins. The second one (``stages``) has almost double the stratigraphic resolution, but some of the ICS stages are clumped to ensure a more even distribution of durations (cf. Miocene and Pliocene).

#It is easier to handle the two timescales with the same functions if the time bin names have identical column names in both tables.

# names of the bins
colnames(tens)[colnames(tens)=="X10"] <- "name"
# names of the bins
colnames(stages)[colnames(stages)=="stage"] <- "name"

# The original data in the Paleobiology Database get their stratigraphic information based on the ``early_interval`` and ``late_interval`` values. The valid entries in these variables come from a list of interval names that convert them to numeric ages and establish connections between the different entries. In the dynamic timescale of Fossilworks (J. Alroy), they are also linked to the 10 million year timescale and the ICS stages (Ogg et al. 2016), without the changes in the Neogene. The ``early_interval`` and ``late_interval`` values designate stratigraphic position in a straightforward way: the ``early_interval`` marks the oldest possible age and the ``late_interval`` the youngest. If a single name suffices to describe the inherent uncertainty of an interval, the ``late_interval`` remains empty.

# Using a complete download of collections from Fossilworks, we compiled a table that resolves these 'interval' entries to the timescales of our interest. This table is the ``stratkeys`` object, which can be attached by running ``data(stratkeys)``.  The rows in this table were then transformed to ``list`` type entries that can be used by the ``categorize()`` function. Similarly to the environmental variables, this can be found in the ``keys`` object. 

# The stratigraphic binning starts with figuring out which numbered bins the ``early_interval`` and ``late_interval`` entries are assigned to. Let's start with the 10 million year bins.

# categorize entries as they are in the lookup table
tenMin <- categorize(dat[ ,"early_interval"], keys$tenInt)
tenMax <- categorize(dat[ ,"late_interval"], keys$tenInt)

# Then the entries have to converted to simple numeric values.
tenMin <- as.numeric(tenMin)
tenMax <- as.numeric(tenMax)

# This code creates two vectors of numeric bin numbers, where ``NA`` entries indicate that the names were not found in the table, and ``-1`` entries indicate empty character strings (where no ``late_interval`` entry is given). As our goal is to retain only those occurrences that have precise enough stratigraphic assignments, we only want to consider those occurrences that have either the same ``tenMin`` or ``tenMax`` number or where ``tenMax`` is ``-1``. This is accomplished by the following steps.

# First, a final, empty vector is defined.
dat$ten <- rep(NA, nrow(dat))

# Then the condition above is expressed indicating which rows have only a single assigned bin number.
tenCondition <- c(
  # the early and late interval fields indicate the same bin
  which(tenMax==tenMin),
  # or the late_interval field is empty
  which(tenMax==-1))

# Finally, those values are copied, where the condition is true.
# in these entries, use the bin indicated by the early_interval
dat$ten[tenCondition] <- tenMin[tenCondition]

# The final object is a column in the data table, where ``dat$ten`` is a single variable of integers that have ``NA`` entries where the collection/occurrence cannot be assigned to a single bin in the time scale. The number of occurrences and collections in each bin can be calculated with ``table()`` or in a single step with the ``binstat()`` function. 
sampTen <- binstat(dat, tax="clgen", bin="ten", coll="collection_no", duplicates=FALSE)

# As every row in ``dat`` represents a species-level occurrence entry, multiple species of the same genus can be registered in every collection. These are only counted as one occurrence, when the ``duplicates`` argument of ``binstat()`` is set to ``FALSE``. 
# The resulting ``data.frame`` can be used to plot the number of occurrences and collections through time.

# the plot
tsplot(stages, boxes="sys", shading="sys", xlim=4:95, ylim=c(0,35000), 
       ylab="Number of entries", xlab="Age (Ma)")
# occurrences
lines(tens$mid, sampTen$occs, lwd=2)
# collections
lines(tens$mid, sampTen$colls, lwd=2, col="blue")
# legend
legend("topright", bg="white", legend=c("occurrences", "collections"), 
       col=c("black", "blue"), lwd=2, inset=c(0.15,0.01), cex=1)

# The following code repeats the entire process for the stratigraphic stages:

# the 'stg' entries (lookup)
stgMin <- categorize(dat[ ,"early_interval"], keys$stgInt)
stgMax <- categorize(dat[ ,"late_interval"], keys$stgInt)

# convert to numeric
stgMin <- as.numeric(stgMin)
stgMax <- as.numeric(stgMax)

# empty container
dat$stg <- rep(NA, nrow(dat))

# select entries, where
stgCondition <- c(
  # the early and late interval fields indicate the same stg
  which(stgMax==stgMin),
  # or the late_intervar field is empty
  which(stgMax==-1))

# in these entries, use the stg indicated by the early_interval
dat$stg[stgCondition] <- stgMin[stgCondition]

# But beware! Stage-level assignments have been a problem in the earliest Paleozoic (Cambrian and Ordovician periods). Therefore, the assignments above are not perfect in these periods: numerous entries are not properly processed based on the interval lookup table.  To make use of these collections in the analyses that follow, we processed these data separately. The necessary files to do these corrections are added to the example GitHub repository and will be updated for future use.
# Cambrian collections were assigned one-by-one by Na Lin for the analysis of studying the diversity dynamics of the Cambrian (Na and Kiessling, 2015). You can download these from:
load(url(
  "https://github.com/divDyn/ddPhanero/raw/master/data/Stratigraphy/2018-08-31/cambStrat.RData"))

# And apply them with the following script:
source(
  "https://github.com/divDyn/ddPhanero/raw/master/scripts/strat/2018-08-31/cambProcess.R")

# For the Ordovician assignments, Wolfgang Kiessling compiled tables on formations and biozones. 

load(url(
  "https://github.com/divDyn/ddPhanero/raw/master/data/Stratigraphy/2018-08-31/ordStrat.RData"))

# You can use these with the following script.

source(
  "https://github.com/divDyn/ddPhanero/raw/master/scripts/strat/2019-05-31/ordProcess.R")

# Now that we assigned the occurrences to stages, as above, the number of sampled occurrences and collections can be calculated with ``binstat()``. 
sampStg <- binstat(dat, tax="clgen", bin="stg", coll="collection_no", duplicates=FALSE)

# And then can be plotted in a similar way.
# the plot
tsplot(stages, boxes="sys", shading="sys", xlim=4:95, ylim=c(0,20000), 
       ylab="Number of entries", xlab="Age (Ma)")
# occurrences
lines(stages$mid, sampStg$occs, lwd=2)
# collections
lines(stages$mid, sampStg$colls, lwd=2, col="blue")
# legend
legend("top", bg="white", legend=c("occurrences", "collections"), 
       col=c("black", "blue"), lwd=2, inset=c(0.15,0.01), cex=1)

# 4. Calculating richness (Figure 1)

# The first figure of the paper depicts changes in genus richness over the Phanerozoic, using the 10 million year intervals with raw and subsampled data. You can calculate the raw results with the basic ``divDyn()`` function.
# raw patterns
ddStages <- divDyn(dat, bin="stg", tax="clgen")

# This function call produces a ``data.frame`` class object, with the calculated metrics as columns. The variables are explained in the function help file ``?divDyn``. Because we have been using positive integer bin numbers, the indices of rows in this table match the bin identifier numbers. Therefore, the corresponding age values are in the same index rows in the ``bins`` table. To visualize the results, we need to plot the timescale first with the ``tsplot()`` function (otherwise it is difficult to match results with time intervals visually) and then we can use ``lines()`` to show the variable in question.
tsplot(stages, boxes="sys", shading="sys", xlim=4:95, ylim=c(0,2800), 
       ylab="Richness (raw data)", xlab="Age (Ma)")
lines(stages$mid, ddStages$divCSIB, col="blue", lwd=2)
lines(stages$mid, ddStages$divSIB, col="#00BB33", lwd=1)
legend("top", inset=c(0.01,0.01), legend=c("raw SIB", "corrected SIB"), 
       col=c("#00BB33", "blue"), lwd=c(1,2), bg="white", cex=1)

# Sampling-standardized values can be calculated with a single function called ``subsample()``.
sqsStagesPlot <- subsample(dat, bin="stg", tax="clgen", coll="collection_no", q=0.7, 
                           iter=100, ref="reference_no",singleton="ref", type="sqs", duplicates=FALSE, 
                           excludeDominant=TRUE, largestColl =TRUE, output="dist", na.rm=TRUE)

# The function above is configured to use the 'Shareholder Quorum Subsampling' (Alroy, 2010a, 2010b) or 'coverage-based rarefaction' (Chao and Jost, 2012) method (``type="sqs"``) that subsamples the data down to an even level of sample coverage (Good, 1953). The former name is usually used to refer to the algorithmic version of the method; latter one to refer to the analytical version. We only implemented the algorithmic version as the complexities of paleontological data make the analytical version less useful in this context. The function is configured to use a reference-based 'singleton' treatment (``singleton="ref"``) for overall sampling correction, excluding dominant taxa from all calculations involving frequencies (``excludeDominant=TRUE``) and with the separate treatment of the largest collection in the time slice (``largestColl=TRUE``), as indicated by Alroy (2010a). Setting ``output`` to ``"dist"`` makes the function return the results of individual subsampling trials. Please take a look at the help files of the functions ``?subsample`` or ``?subtrialSQS`` if you want to know more.

# You can compare one trial result (in this case the 51st) with the original time series with the following code:
tsplot(stages, boxes="sys", shading="sys", xlim=4:95, ylim=c(0,2800), 
       ylab="Richness (corrected SIB)" , xlab="Age (Ma)")

lines(stages$mid, ddStages$divCSIB, col="blue", lwd=2)
lines(stages$mid, sqsStagesPlot$divCSIB[ ,51], col="#00000088", lwd=2)
legend("topleft", bg="white", legend=c("raw data", "subsampling trial"),
       col=c("blue", "#00000088"), lwd=3, inset=c(0.01, 0.01))

# You can either let the function average the trial results by setting the ``output`` argument accordingly, or you can do it yourself. Here are the results of the 100 trials calculated above.
# only plot
tsplot(stages, boxes="sys", shading="sys", xlim=4:95, ylim=c(0,1000), 
       ylab="Subsampled richness (corrected SIB)", xlab="Age (Ma)")

# loop through all trial results
for(i in 1:ncol(sqsStagesPlot$divCSIB)){
  lines(stages$mid, sqsStagesPlot$divCSIB[,i], col="#00000088")
}

# the mean of the trial results
meanRes <- apply(sqsStagesPlot$divCSIB, 1, mean, na.rm=T)
lines(stages$mid, meanRes, col="#CC3300", lwd=2)

# 5. Analyzing the taxonomic rates

# This study focuses on turnover rates rather than richness estimates. The prototypes for these are the analyses presented by Alroy (2008). Here we provide the detailed analytical code to reproduce these using the per capita rates (Foote, 1999). These are the most used in the literature, even though they were critiqued by Alroy (2014) for their accuracy. The stratigraphic resolution in this is example was at the level of stages. All necessary functions are included in the ``phanDyn.R`` file, which is deposited on GitHub.

# load the necessary functions
source("https://github.com/divDyn/ddPhanero/raw/master/scripts/1.0.1/phanDyn.R")

# The analyses in the following part of this section can be repeated with any set of extinction rate, origination rate and richness time series. To prepare the generalized implementation of the following code in a function (Section 5.3), the necessary variables are renamed so the same analyses can be rerun on different series if they are named appropriately. Variables from the timescale tables are also necessary.

# the name of the overall data frame (already created)
ext <- ddStages[ , "extPC"] # extinction rates
ori <- ddStages[ , "oriPC"] # origination rates
div <- ddStages[ , "divCSIB"] # diversity (richness) series
dur <- stages[ , "dur"] # durations of time intervals
age <- stages[ , "mid"] # time interal midpoints
name <- stages[ , "name"] # names of the time intervals

### A. Is the pulsed model of turnover supported by the instantaneous rates? 

# The taxonomic rates currently implemented in the package converge on the per capita rates (Foote, 1999) as sampling completeness approaches 1. The original definition of the per capita rates expresses the intensity of turnover as per lineage-myr, which means that the log proportions are divided (normalized) by the durations of the time intervals. Implicit in these equations is that taxonomic turnover happened continuously in the bin, which has been suggested to be an invalid assumption, at least for extinctions (known as the pulsed model: Foote, 2005). If a 'pulsed' model is correct, the normalization of rates by bin duration is not only unnecessary but plain wrong. A 'pulsed' model is supported by the absence of correlations between turnover rates and bin durations (Alroy, 2008). With normalization a negative correlation between rate and duration may arise (Alroy, 2008). Rates calculated this way are sometimes characterized as 'instantaneous'.

# Using the ``pulseCont()`` function we prepared tests for correlations between interval durations and rate values with and without the normalization. It uses tests of Spearman's rank-order correlations to assess these relationships. The function can be found in the ``phanDyn.R`` file, and it takes a rate series and vector of bin durations as arguments.
pulseCont(ext, dur, alpha=0.01)

#The ``est`` element of the output list contains the correlation coefficients, ``p`` the *p*-values, and the ``sig`` element indicates significant results at the *p* < 0.01 level. As only the normalized rates are significantly correlated with bin durations, it is likely that normalization strengthens the association between the two variables. This suggests that for extinctions (based on the raw, stage-level per capita rates) the 'pulsed' model turnover is more appropriate.

### B. Are the taxonomic rates declining? 

#The decline of taxonomic turnover rates is one of the most robust findings of paleobiology and has been reported many times in the literature (e.g. Raup and Sepkoski, 1982; Sepkoski, 1998, Bambach et al, 2004; Alroy, 2008). Therefore, we expect both origination and extinction rates to decline over time. 
#In general, we have to be cautious about the earliest and latest parts of the time series. First, these rates cannot be estimated for the first and last bins (some even not for the second and the second last), as it is impossible to count taxa with certain temporal patterns of occurrences in these intervals (e.g. three-timers). The so-called edge-effects (Foote 2000) also bias the per-capita rates systematically. The stratigraphy of the Cambrian and the Ordovician intervals remains problematic, while we also know that higher taxonomic entities originated in these intervals that separate them from the rest of the Phanerozoic. 
#Therefore, we decided to copy the analytical parameters of Alroy (2008), and the declining pattern was also assessed separately for the post-Ordovician interval. The following intervals were applied in the assessment:

# total decline interval in Ma
maxAgeDecline <- 540
minAgeDecline <- 20
# mid dates of post-Ordovician time bins (both time scales) are younger than
postDate <- 443

# To retain the match between the bin numbers in the time scale table and the indices of the results, we defined logical vectors that replace values that are out of this interval with ``NA`` entries. 

# logical vectors indicating what is necessary
# Cambrian included 
indNAlong <- !(age<=maxAgeDecline & age>=minAgeDecline)
# post Ordovician
indNApostord <- !(age<=postDate & age>= minAgeDecline)

# You can relate these values to the estimated ages in ``bins`` and ``stages`` objects. It is good practice to visualize the variables before carrying out the correlation tests.
# actual variables used in the correlation tests:
extVarLong <- ext
oriVarLong <- ori
extVarLong[indNAlong] <- NA
oriVarLong[indNAlong] <- NA

# plotting
# maximum y value
nMaxRate<-max(c(extVarLong, oriVarLong), na.rm=T)
# actual plot
tsplot(stages,  boxes="sys", shading="sys", ylim=c(0, nMaxRate*1.05), 
       ylab="Turover rates", xlim=c(530,0), xlab="Age (Ma)")
lines(age, extVarLong, col="red")
lines(age, oriVarLong, col="blue")
legend("topright", legend=c("extinctions", "originations"), col=c("red", "blue"), 
       lwd=c(1,1), bg="white", inset=c(0.01, 0.01))

# Some patterns of a decline are evident from viewing these series, but we need to test them. We can create a ``2`` by ``4`` matrix to hold results of correlations against time (coefficients and *p*-values) that will be calculated for each of the series.

# create a table from the decline information
decMat <- matrix(NA, ncol=2, nrow=4)
colnames(decMat)<- c("correlation","p-value")
rownames(decMat)<- c("extinction", "post-Ordovician extinction", 
                     "origination", "post-Ordovician origination")	

# First, the total declines (``extVarLong`` and ``oriVarLong``) are assessed.
# is the extinction rate declining
extDecline <- cor.test(age, extVarLong, method="spearman")
decMat[1, 1] <- extDecline$estimate
decMat[1, 2] <- extDecline$p.value

# is the origination rate declining
oriDecline <- cor.test(age, oriVarLong, method="spearman")
decMat[3, 1] <- oriDecline$estimate
decMat[3, 2] <- oriDecline$p.value

# Then the post-Ordovician declines are assessed.
# extinction
extPostOrd <- extVarLong
extPostOrd[indNApostord] <- NA
extDeclinePostOrd <- cor.test(age, extPostOrd, method="spearman")
# save	
decMat[2, 1] <- extDeclinePostOrd$estimate
decMat[2, 2] <- extDeclinePostOrd$p.value

# origination
oriPostOrd <- oriVarLong
oriPostOrd[indNApostord] <- NA
oriDeclinePostOrd <- cor.test(age, oriPostOrd, method="spearman")
# save
decMat[4, 1] <- oriDeclinePostOrd$estimate
decMat[4, 2] <- oriDeclinePostOrd$p.value

# The results of the test above are in the ``decMat`` object, clearly suggesting patterns of an overall decline for both extinctions and originations, although this might not be true for the post-Ordovician parts of the series.
round(decMat, 4)

## 5.2. Detrending the series

# The definition of the rate distribution, identification of outlying values and implementation of accurate tests of cross correlations between the time series assume that the series are stationary. To get closer to meeting this assumption, the rate series have to be detrended. There are multiple ways of fitting a trend to the time series. You can fit a polynomial function, model it as an ARIMA process, or apply a scatterplot smoother (e.g. LOESS) to the values. 

# Among these options, the last is the simplest one and it has the fewest assumptions about the underlying processes. This is a very useful property, as it decreases the chance of inappropriate fit when the same analytical code is run on different time series, or when long-term trends are not significant. This section starts by going through how this procedure can be applied to the time series of originations and extinctions without transforming the data (see section 5.3 for other options).

# LOESS relies on a bandwidth parameter (or ``span``) that describes how much smoothing should be applied to the series, which is usually considered to be an arbitrary parameter.  Luckily, there are ways to search for an optimal setting. An easily applicable implementation is included in the ``fANCOVA`` package (Wang, 2010), which you can download from the CRAN servers with the following line of code:

# The relevant function requires the time series to be free of missing values with the corresponding ages. You can get rid of these in a structured way by executing these commands:

# extinctions
extMiss <- !is.na(extVarLong)
transExtNoNA <- extVarLong[extMiss]
ageExt <- age[extMiss]

# originations
oriMiss <- !is.na(oriVarLong)
transOriNoNA <- oriVarLong[oriMiss]
ageOri <- age[oriMiss]

# After running this snippet, ``transExtNoNA`` includes the extinction rate values and ``ageExt`` has the corresponding ages. The same applies to ``transOriNoNA`` and ``ageOri`` but for originations. Doing the actual smoothing is just a single step with ``fANCOVA``.
# the models
extModel <- fANCOVA::loess.as(ageExt, transExtNoNA, degree = 1, 
                              criterion = "aicc", user.span = NULL, plot = FALSE)
oriModel <- fANCOVA::loess.as(ageOri, transOriNoNA, degree = 1, 
                              criterion = "aicc", user.span = NULL, plot = FALSE)

# These settings will make the function use corrected AICs to define the optimal smoothing of the curves. To get the actual values, predictions must be calculated from these model objects. Running the ``predict()`` function on them will output the predicted values (i.e. the trend).

# predictions for extinctions
predict(extModel) 

# There is a problem with this result, however, namely that certain predictions where the original time series had ``NA`` values are not present. This leads to misalignment with the other time series and numerous future problems. Therefore, it is better practice to coerce the output of missing values in the prediction, by specifying the ``x`` (time) coordinates for the predictions. These are in the original ``age`` vector and can be used this way:
predExt <- predict(extModel, newdata=data.frame(x=age)) # extinctions
predOri <- predict(oriModel, newdata=data.frame(x=age)) # originations

# The resulting objects have the same number of values as the original series.
length(predExt)
length(predOri)

# You can draw the predicted values with ``lines()``.
lines(age, predExt,col="red", lwd=2) # extinctions
lines(age, predOri,col="blue", lwd=2) # originations

tsplot(stages,  boxes="sys", shading="sys", ylim=c(0, nMaxRate*1.05), ylab="Turnover rates", 
       xlim=c(530,0), xlab="Age (Ma)")
lines(age, extVarLong, col="red")
lines(age, oriVarLong, col="blue")
legend("topright", legend=c("extinctions", "originations"), col=c("red", "blue"), lwd=c(1,1),
       bg="white", inset=c(0.01, 0.01))
lines(age, predExt, col="red", lwd=2)
lines(age, predOri ,col="blue", lwd=2)

# The detrended values are then the residuals with multiplicative decomposition.
detExt <- extVarLong/predExt
detOri <- oriVarLong/predOri

# Following Alroy (2008), only the post-Cambrian values were used in further analyses, as the Cambrian rates appear to be unusually high in all cases.
# interval for detrending in ma	
maxAgeDetrend <- 475
minAgeDetrend <- 20
# indices and variables
indNAshort<- !(age<=maxAgeDetrend & age>=minAgeDetrend)

# recreate the variables
detExtShort <- detExt
detOriShort <- detOri
detExtShort[indNAshort] <- NA
detOriShort[indNAshort] <- NA

# You can plot the residuals with ``lines()``.
# for setting the y-axis range
detMax <- max(c(detExtShort,detExtShort), na.rm=T)
# plot		
tsplot(stages,  boxes="sys", shading="sys", ylim=c(0,detMax*1.05), 
       ylab="Detrended turnover rates", xlim=c(530,0), xlab="Age (Ma)")
lines(age, detExtShort, col="red" ) # extinctions
lines(age, detOriShort, col="blue" ) # originations
legend("topleft", legend=c("extinctions", "originations"), col=c("red", "blue"), 
       lwd=c(1,1), bg="white", inset=c(0.2, 0.01))

# As mentioned before, there are other ways to do the detrending of the series, some  of which we also implemented (see section 5.3 for options). You are invited to inspect these within the ``analyzedMetrics()`` function that can be found in the ``phanDyn.R`` file.

### C. Is the distribution of rates lognormal?

# Mass extinctions have always been diagnosed by analyzing the distributions of the rates. Originally, these were just compared to the point estimation confidence-interval of a linear decline model (Raup and Sepkoski, 1982), which was already criticized back then (Quinn, 1983). Bambach et al. (2004) fitted a LOESS model to the extinction rates, and analyzed the distribution of its residuals (similar to what we just did, assuming a local background process). Alroy (2008) suggested detrending the rates with exponential functions. He analyzed the distribution of the rates first and then pointed to potential outliers. Here is the distribution of the detrended extinction rate series with a kernel density estimator:
# extinctions histogram
hist(detExtShort , breaks=30, xlim=c(0,5), col="red", 
     xlab="Detrended extinction rate values", main="")
# kernel density estimator
den <- density(detExtShort [!is.na(detExtShort )])
lines(den$x, den$y, lwd=2)

# And here is the same plot for originations.
# histogram
hist(detOriShort, breaks=30, xlim=c(0, 5), col="blue", 
     xlab="Detrended origination rate values", main="")
# kernel density estimator
den<-density(detOriShort[!is.na(detOriShort)])
lines(den$x, den$y, lwd=2)

# As the extinction rate distributions tend to be right-skewed and they are 0-bounded, analyzing the logarithms of the rates, or their square roots make more sense. The question behind the analysis of rate distributions is whether we can distinguish between two processes of extinction in terms of mass extinction episodes and background extinctions, or whether the two just at different positions along a spectrum. A lognormal distribution suggests that there is only a difference in magnitude between these intervals, which has to be assessed. Therefore, the function checks whether the rates can come from a lognormal distribution or not, which is implemented by using Shapiro-Wilk tests. 
# are logged data normal? - extinction
extVar <- log(detExtShort) 
extVar[is.infinite(extVar)] <- NA # omit infinites
pShap <- shapiro.test(extVar)$p.value
names(pShap)[1] <- "ext"

# are logged data normal? - origination
oriVar <- log(detOriShort)
oriVar[is.infinite(oriVar)] <- NA	# omit infinites
pShap <- c(pShap,shapiro.test(oriVar)$p.value)
names(pShap)[2] <- "ori"

# the p-values of the tests:
pShap

# These results indicate that at the stage-level resolution and using the per capita rates (Foote 1999), both origination and extinction rate series have lognormal distributions, suggesting that mass extinction intervals are not qualitatively different from background turnover.

### D. Which time slices are outliers from the distribution?

# The exact definition of a 'mass extinction interval' has become somewhat blurry over the years.  As rates fit a lognormal distribution quite well, it appears likely that background and mass extinction intervals are similar in qualitative terms, they are just at different positions along a spectrum. Even if this is the case, cataclysmic events (i.e. mass extinctions) will fall in the upper tail of the extinction rate distribution, which we can separate with non-parametric methods.
# The literature generally agrees that the largest 'mass extinction' was at the Permian/Triassic boundary, which we can reassess with a following line of code.
largest <- name[which(max(detExtShort,  na.rm=T)==detExtShort)]
largest

# Which is the latest Permian interval, and is indistinguishable from the end-Permian mass extinction. We can also identify and contrast potential outliers with R's ``boxplot()`` function. 
boxp <- boxplot(detExtShort, ylab="Detrended extinction rates")
outliers <- name[as.numeric(names(boxp$out))]
outliers

#This suggests that without making any assumptions about the extinction rate distribution, four mass extinctions can be identified in the raw, stage-resolution data and the per capita rates. Among these, the end-Permian (Changhsingian), end-Triassic (Rhaetian) and end-Cretaceous (Maastrichtian) mass extinctions ('Big Three' in Alroy, 2008) are the most studied and usually represent the highest values in detrended extinction rate series. The presences of these three events are assessed with different rate calculation methods using the function presented in section 5.3. The Katian value is most likely associated with the end-Ordovician event.

### E. Can we find traces of equilibrial dynamics in the series?

#The quest to detect equilibrial patterns of diversity dynamics is rooted in island biogeography. Equilibrial dynamics means that diversity is bounded, but it does not necessarily mean that carrying capacity is stable through time. It rather points to the fact that there is an attractor in the richness dimension, pulling diversity up, when it is relatively low, and depressing it when it is relatively high (Alroy, 2010b).
#Alroy (2008) argued that equilibrial dynamics should manifest in three cross-correlations: 
  
#  * Higher origination rates in time slice *i* should lead to higher diversity in time slice *i*+1.

# * Higher diversities in time slice *i* in general should lead to higher extinction rates in time slice *i*+1

# * Higher extinction rates in time slice *i* should lead to higher origination rate in slice *i*+1.

# We have already detrended the turnover rates, but we still need to detrend the richness values. We follow the same basic procedure.

# select the same interval as for turnover rates
divVarLong <- div
divVarLong[indNAlong] <- NA

# This can be plotted with:

# plotting the logged series
nMaxDiv <- max(divVarLong, na.rm=T)
nMinDiv <- min(divVarLong, na.rm=T)
tsplot(stages,  boxes="sys", shading="sys", ylim=c(nMinDiv*0.95, nMaxDiv*1.05), 
       ylab="Richness", xlim=c(530,0), xlab="Age (Ma)")
lines(age, divVarLong, col="black")		

# After this step, the LOESS regression is applied to the data. 

divMiss <- !is.na(divVarLong)
transDivNoNA <- divVarLong[divMiss]
ageDiv <- age[divMiss]

divModel <- fANCOVA::loess.as(ageDiv, transDivNoNA, degree = 1, 
                              criterion = "aicc",  user.span = NULL, plot = FALSE)

# The predictions of this model are then calculated. 

# predicted
transPredict <- predict(divModel, newdata=data.frame(x=age))
lines(age, transPredict, lwd=2)	

# actual prediction
transPredict <- predict(divModel, newdata=data.frame(x=age))
tsplot(stages,  boxes="sys", shading="sys", ylim=c(nMinDiv*0.95, nMaxDiv*1.05), 
       ylab="Richness", xlim=c(530,0), xlab="Age (Ma)")
lines(age, divVarLong, col="black")
lines(age, transPredict, lwd=2)	

# The residuals are taken and are rescaled to the original magnitude, using the mean of the original series.
transResid <- divVarLong/transPredict  # multiplicative decomposition
detDiv<- mean(divVarLong, na.rm=T)*transResid # rescaling

# The series was then subsetted to include only those values that are in the detrended turnover rate series.

# limit to the same part as the turnover
detDivShort <- detDiv
detDivShort[indNAshort] <- NA

# The resulting series is comparable to the original series, but without the increasing trajectory.
# y ranges
tsplot(stages,  boxes="sys", shading="sys", ylim=c(nMinDiv*0.95, nMaxDiv*1.05), 
       ylab="Richness", xlim=c(530,0), xlab="Age (Ma)")
lines(age, divVarLong, lty=1)
lines(age, detDivShort, col="black", lty=2)
legend("top", inset=c(0.01, 0.01), legend=c("original", "detrended"), lty=c(1,2), 
       bg="white")

# After the diversity series is detrended, another small function ``dynamics()`` is applied to calculate cross correlation patterns at the chosen lag between the detrended extinction rate, origination rate and diversity variables. You can find this in the ``phanDyn.R`` file. ``ext``, ``ori`` and ``div`` are the original detrended series (extinction, origination and diversity, respectively), and ``extPlus``, ``oriPlus`` and ``divPlus`` denote the shifted series. For instance, a positive correlation between ``ext`` and ``oriPlus`` indicates that high extinction rate values are usually followed by high origination rate values in the next bin. Correlations between ``ext`` and ``extPlus`` indicate autocorrelations. A single run of this function produces the following results, with the alpha level of ``0.01``.

dyn <- dynamics(ori=detOriShort, ext=detExtShort, div=detDivShort, method="spearman", 
                l=1, alpha=0.01)
# the $sig element returns significant components
dyn$sig

## 5.3. Single analysis function

# After the basic analytical script was completed to answer the questions in sections 5.1 and 5.2 (above) a single function was prepared that takes any origination rate, extinction rate and richness time series and performs the analyses that we presented above.  This function ``analyzeMetrics()`` can be found in the ``phanDyn.R`` file. The function requires a single ``data.frame`` object that has to include variables with the three time series, the bin durations, bin midpoint ages and names. All this information can be compiled by concatenating the time scale object and the output ``data.frame`` of the ``divDyn()`` function.
metrics <- cbind(ddStages, stages)

# The arguments ``ext`` (extinction), ``ori`` (origination), ``div`` (richness/diversity), ``age``, ``dur`` (duration) and ``name`` refer to the column names of the required variables. 
# The function was implemented to return whether the 'pulsed' or 'continuous' model is supported, and to apply normalization with bin durations, if it is chosen so by the user. Normalized rates are only used in further analyses, when a correlation between rates and bin durations was evident in the non-normalized data and when the argument ``normalize`` is set to ``TRUE``. In case both correlation tests are significant, the pulsed model is used as the default. Setting this argument to ``FALSE`` will coerce the calculations to use the input time series, using the 'pulsed' model throughout. 

# As mentioned in section 5.2, there are multiple ways to detrend the series. We implemented some of these methods, the procedure can be configured by adjusting the ``detrend``, ``transform`` and ``additive`` arguments of the ``analyzedMetrics()`` function. The ``detrend`` argument specifies a switch between the different detrending options. Setting this to ``"loess"`` will run the detrending process shown in section 5.2 based on LOESS regression with the ``fANCOVA`` package (Wang, 2010). The option ``"linear"`` will fit a linear model to all three series. Setting the argument to ``"arima"`` will install and use the ``forecast`` package (Hyndman and Khandakar, 2008) to fit ARIMA models with the ``auto.arima()`` function.  The argument ``transform`` specifies which transformation should be applied to the time series before the decomposition takes place. Setting this argument to ``FALSE`` will use the original series, ``"log"`` applies logarithm transformation and then exponentiation, ``"sqrt"`` applies square root transformation and then the squaring of the predicted and detrended values. The logical argument ``additive`` indicates whether additive or multiplicative decomposition is used to remove the trend. 

# The function also has additional arguments that enable the plotting of time series (``plot=TRUE``) and the output of messages and warnings (``feedback=TRUE``).  If ``plot`` is set to ``TRUE`` a single four-panel plot is produced, where the panels depict: 1. rate values, 2. richness values, 3. detrended rates , 4. transformed richness values (identical to 3., if no transformation happens).

res <- analyzeMetrics(metrics, ext="extPC", ori="oriPC", div="divCSIB", 
                      age="mid", dur="dur", name="name", normalize=FALSE, 
                      plot=TRUE, feedback=FALSE)

#The ``list`` class output of the function includes the objects that were introduced earlier in sections 5.1 and 5.2.
res

# 6. Applying the analyses to multiple series

# The best thing about the rate and diversity estimators is also the worst thing: they are in continuous development. Although there is evidence to support the better applicability of some methods (Alroy, 2014; 2015), different researchers may favor different solutions to answer scientific hypotheses. Therefore, we are not trying to impose the use of any methods, but rather intend to make these calculations available to everybody and demonstrate the effects of methodological choices on the results. 

# For instance, results depend on the chosen equations to calculate the taxonomic turnover series. Although the per capita (Foote, 1999) rates were the most frequently used method in the past two decades, using these equations for the estimation of the rates can lead to systematic errors close to the edges of the time series (i.e. edge effects; Foote, 2000). Other methods (Alroy, 2008; 2014; 2015 ) were developed to be unaffected by this phenomenon. Nevertheless, even within the series, the different methods will lead to considerably varying values that can have an effect on the results used to test overarching hypotheses.

# Therefore, we decided to redo the analysis presented in section 5 using different metrics of turnover and methods of sampling standardization, to assess how robust the general findings are in the face of different methodologies. Sets of time series were drafted based on two different resolutions (10my bin and stages), three different data treatments (using raw data, Classical Rarefaction and Shareholder Quroum Subsampling) and four different rate calculation methods (per capita rates, corrected three-timer rates, gap-filler equations and second-for-third substitution rates). Metrics published earlier were not considered in the analyses as they have known problems with accuracy (Foote, 1994). Equilibrial dynamics were tested using the corrected SIB diversity values. 

## 6.1. Calculating time series with multiple methods

# The candidate metrics are calculated with the different resolutions and data treatment options. First, the calculations with the 10 my time scale are implemented. 

# Before the calculation of the actual values can take place, the desired level of sampling intensities has to be determined for the sampling standardization processes. Classical Rarefaction (CR) is the oldest of the subsampling methods (Raup, 1975). Diversity curves drafted with this method are strongly dependent on sampling intensity: Alroy (2010b) has shown that the method leads to richness curves that flatten as the quota (desired sampling intensity in occurrences) decreases. Despite the fact that CR has its own limitations for richness estimation, it is still one of the most widely used sampling standardization protocol in the literature, and it produces comparable results to SQS with our data. 

# The quota for CR is set so that a complete time series can be produced without creating time slices with 'failed' subsampling, where there are not enough sampled occurrences. These were already calculated and are present in the ``sampTen`` and ``sampStages`` objects. 

# Let's first look at the number of occurrences in the 10 myr stage-results. We need to find out which bins are the least sampled to figure out a good subsampling quota. To do this, it is very useful to assign the bin identifiers to the number of occurrences first (names attribute), and then we can put the occurrences in ascending order.

binOccs <- sampTen$occs
names(binOccs) <- paste("bin", rownames(sampTen), sep="")
# in ascending order
sort(binOccs)

# This indicates that the first bin is disproportionately poorly sampled. This is unlikely to convey important information for most of the time series. At the same time, subsampling the whole series to this low sampling level would destroy a huge amount of information. However, the second lowest level of about 4,800 occurrences (``bin16``, Devonian 5) looks like a decent level of sampling and can be applied to almost the whole time series.

# The same reasoning can be applied to the stage-level data.

stgOccs <- sampStg$occs
names(stgOccs) <- paste("stg", rownames(sampStg), sep="")
sort(stgOccs)

# In this case, the number of occurrences in a bin increases gradually, and multiple poorly sampled bins are in the middle of the series. Using different quotas would create different results, for the sake of simplicity, it is probably best to restrain ourselves to a lower quota of 1,100 occurrences for the stage-level analyses. 

# For SQS, the subsampling configuration discussed in Section 4 stands here, too. A major advantage of SQS to CR is that it gives you more or less the same curve regardless of the quorum, as long as the quorum isn't extremely low (< 0.4 or so, Alroy 2010b).  

# The large quantity of data and the relatively small subsample sizes indicate that one must carry out a high number of iterations for the estimates to stabilize. The results presented in this section are based on 500 iterations, which will take a considerable amount of time to run, but you can decrease this parameter at will by setting the ``iter`` argument of the ``subsample()`` function.

# 1. raw patterns
ddTen <- divDyn(dat, bin="ten", tax="clgen")
# 2. CR
crTen <- subsample(dat, bin="ten", tax="clgen", coll="collection_no", q=sort(binOccs)[2], iter=500, # RGS: code updated here to repeat the Kocsis approach but using new quotas based upon filtered datasets
                   duplicates=FALSE, na.rm=TRUE)
# 3. SQS
sqsTen <- subsample(dat, bin="ten", tax="clgen", coll="collection_no", q=0.5, iter=500, # RGS: quorum reduced to 0.5 (as Kocsis et al. report is fine), in keeping w. Boag et al. Curr Bio Fig S2, to minimize failed bins at quorum of 0.7
                    ref="reference_no", singleton="ref", type="sqs", duplicates=FALSE, excludeDominant=TRUE,
                    largestColl =TRUE, na.rm=TRUE)

# Then the calculations were repeated for the stage-level resolution.

# 1. raw patterns
ddStg <- divDyn(dat, bin="stg", tax="clgen")
# 2. CR
crStg <- subsample(dat, bin="stg", tax="clgen", coll="collection_no", q=sort(stgOccs)[2], # RGS: code updated here to repeat the Kocsis approach but using new quotas based upon filtered datasets
                   iter=500, duplicates=FALSE, na.rm=TRUE)
# 3. SQS
sqsStg <- subsample(dat, bin="stg", tax="clgen", coll="collection_no", q=0.5, # RGS: quorum reduced to 0.5 (as Kocsis et al. report is fine), in keeping w. Boag et al. Curr Bio Fig S2, to minimize failed bins at quorum of 0.7
                    iter=500, ref="reference_no", singleton="ref", type="sqs", duplicates=FALSE,
                    excludeDominant=TRUE, largestColl=TRUE, na.rm=TRUE)

# In total, we have 6 different result objects. 

## 6.2. Running the analysis script on the different sets of time series

# The next thing is to organize the results (i.e., create identifiers of the different time series) so the analyses described in section 5 can be iterated. Each run of the analytical script needs three time series (origination, extinction, richness), which can be identified with the appropriate result table names (stratigraphic resolution, data treatment) and the variable names (metric type). The following object ``comb`` is created to organize these combinations. 

# combination table
# types of rates
comb <- matrix(
  c(
    "extPC", "oriPC", "divCSIB", # per capita rates
    "extC3t", "oriC3t", "divCSIB", # corrected 3t rates
    "extGF", "oriGF", "divCSIB", # gap-filler rates
    "ext2f3", "ori2f3", "divCSIB" # second-for-third-substitution rates
  ), 
  ncol=3, nrow=4, byrow=T)
rownames(comb) <- c("PC", "C3t", "GF", "2f3")
comb <- comb[rep(1:4, 6), ]

# the result matrices
sourceVar<-rep(c("ddTen", "crTen", "sqsTen", "ddStg", "crStg", "sqsStg"),each=4)

# timescale objects
scale <- rep(c("tens", "stages"), each=12)
# combine everything together
comb <- cbind(sourceVar, scale, comb)
colnames(comb) <- c("source", "timescale", "ext", "ori", "div")

# the names of the results (rownames)
subtype <- rep(rep(c("raw", "cr", "sqs"), each=4),2)
rownames(comb) <- paste(subtype, rownames(comb), sep="")
rownames(comb) <- paste(rownames(comb), rep(c("10my", "stages"), each=12), sep="_")
comb

# In this table, every row corresponds to the arguments of a certain set of time series. For instance, the first row points to the raw per capita rates with the 10 myr stratigraphic resolution. Now, all that remain is to initialize and iterate the analyses using the different sets of time series. In order to display the rates together, they have to be saved in separate containers.

# matrices to hold rates for later plotting
extDatTen <- data.frame(ten=1:49) # extinctions, 10my
oriDatTen <- data.frame(ten=1:49) # originations, 10my
extDatStg <- data.frame(stg=1:95) # extinctions, stages
oriDatStg <- data.frame(stg=1:95) # originations, stages

# The first two ``data.frame`` objects correspond to the 10my bin resolution and the second one to the stage-level resolution. The following code applies the function to the different time series in a ``for`` loop (for transparency).

#! # initialize pdf, if you want to
#! pdf("2018-08-21_marineAnimals2_redo.PDF", 17,14)
# iterate through all cases
for(i in 1:nrow(comb)){
  # name of the set
  case <- rownames(comb)[i]
  # the results matrix and the timescale object combined
  metrics <- cbind(get(comb[i, "timescale"]), get(comb[i, "source"]))
  
  # save rates for later, depending on the resolution
  # 10 my
  if(comb[i, "timescale"] == "tens"){
    oriDatTen[[case]] <- metrics[, comb[i, "ori"]]
    extDatTen[[case]] <- metrics[, comb[i, "ext"]]
  }
  # stages
  if(comb[i, "timescale"] == "stages"){
    oriDatStg[[case]] <- metrics[, comb[i, "ori"]]
    extDatStg[[case]] <- metrics[, comb[i, "ext"]]
  }
  # the analytical function
  res <- analyzeMetrics(metrics, ext=comb[i, "ext"], ori=comb[i, "ori"], 
                        div=comb[i, "div"], age="mid", dur="dur", name="name", normalize=TRUE, 
                        plot=FALSE, feedback=FALSE, detrend="loess", transform=FALSE, additive=FALSE)
  
  # save in global namespace with unique name
  assign(case, res)
  

  # add the name to the plot
  #! par(mfrow=c(1,1))
  #! mtext(side=3, text=case, line=-2, cex=3)
}

#! dev.off()

#With this snippet, each result will be stored as a list class object in the global namespace, with the name of the row in ``comb`` that contains the arguments. 

#All lines commented with ``#!`` are part of the embedded plotting functionality. If you uncomment these lines (delete ``!`` too!) and set ``plot=TRUE`` in the ``analyzeMetrics()`` function call, a single .pdf file will be produced that will show the four-panel figure presented in section 5.3 for every set of time series (row in ``comb``) on a separate page. You can take look at this ``.pdf`` file by following this link:
#```
#https://github.com/divDyn/ddPhanero/blob/master/export/1.0.1/detrending.pdf
#```

## 6.3. Summarizing the results (Table 3)

# To get an idea of the generality of the results, the support for the individual hypotheses can be summarized in a single display item. We prepared a function to extract correlation coefficient estimates and *p*-values from the individual objects. Two temporary tables were prepared - one for the estimates and another one for the significance values. The idea behind this approach is to use the first table to present the actual values and use the table of *p*-values to format the first table (using conditional formatting in MS Excel). The function to gather the relevant information from the lists can be inspected in the ``phanDyn.R`` file (``extractVals()`` function). 
# the shown values
values <- extractVals(rownames(comb), pvals=FALSE)

# This is a fairly large table that includes every case-related results in a column. For correlation tests, the table denotes the coefficient estimates. For the presence of mass-extinction intervals, the table shows binary values. 
# As the iteration through the output object is the same for the p-values and the estimates, it was a straightforward choice to use the same function to extract the *p*-values. Setting the ``pvals`` argument of this function to ``TRUE`` will extract the *p*-values, where another they are also present.

# the p values
pvals <- extractVals(rownames(comb), pvals=TRUE)

# Note that the entries for the presences of mass extinctions in this table there are no significance values. The table was compiled to be used for conditional formatting and 0.002 values indicate the presence of a mass extinction, 0.02 indicates its absence.

# The support or rejection of hypotheses depend on the *p*-values. The results based on the 10 myr-scale results have the following values (rounded and transposed for better readability):

round(t(pvals[1:12,]),3)

# The stage-level *p*-values can also be extracted in a similar way: 
round(t(pvals[13:24,]),3)

# The tables of *p*-values and estimates can be combined using conditional formatting to render a comprehensible table. The following formatting steps were applied to these tables that were later fed to an MS Excel spreadsheet. 

# round most values
valuesRounded <- round(values,2)
# except for p-values
valuesRounded[,
              c("extinctions log-normal (p-values)",
                "originations log-normal (p-values)")] <-         
  round(values[,c(
    "extinctions log-normal (p-values)",
    "originations log-normal (p-values)")],3)

#transform to character
#valuesRounded <- as.data.frame(valuesRounded)
for(i in 9:13){
  valuesRounded[ ,i] <- as.character(valuesRounded[,i])
  valuesRounded[valuesRounded[ ,i]=="1",i] <- "yes"
  valuesRounded[valuesRounded[ ,i]=="0",i] <- "no"
}

# use inequality signs where values are very low
valuesRounded[
  valuesRounded[,"extinctions log-normal (p-values)"]=="0",
  "extinctions log-normal (p-values)"] <- "<0.001"
valuesRounded[
  valuesRounded[,"originations log-normal (p-values)"]=="0",
  "originations log-normal (p-values)"] <-  "<0.001"

# In the actual paper, Table 3 was compiled after the transposition of the estimate and *p*-value tables, and the separation of both tables to 10 myr time scale and stage-level timescale subsets.
# transpose everything
tValues<- t(valuesRounded)
tP <- t(pvals)

# separate based on resolution
# 10 myr
valBin <- tValues[,1:12]
pBin <- tP[,1:12]
# stages
valStage <- tValues[,13:24]
pStage <- tP[,13:24]

# These four tables were then fed to the Excel spreadsheet we prepared to do the conditional formatting of the ``valBin`` and ``valStage`` tables. This MS Excel file (``conditionalTable.xlsx``) can also be found at the GitHub repository of the example. 	

## 6.4. Figure of rates with multiple methods (Figure 2)

# To show the total variation in the taxonomic rate series, the origination and extinction rates were rendered at the two different resolutions. Four panels were drawn, each panel having 12 time series. The quantiles of the value distributions in the independent time slices were connected with the ``shades()`` function to provide backgrounds.

# Then the panels in sequence:

par(mfrow=c(2,2))
# extinctions
# 1st panel - 10myr bins
par(mar=c(2, 5.1, 4.1, 1)) # margin adjustment
# plot
tsplot(stages, boxes="sys", shading="sys", ylim=c(0, 3), ylab="Extinction rates", 
       xlim=4:95, plot.args=list(axes=F, cex.lab=0.8), xlab="",labels.args=list(cex=0.8))
axis(2, cex.axis=0.8)
shades(tens$mid, as.matrix(extDatTen[,-1]), col="red") # background
for(i in 2:ncol(extDatTen)) 
  lines(tens$mid, extDatTen[,i] , lwd=0.5) # extinctions - BIN
mtext(line=1, text="10 my bins", side=3, cex=1) # show resolution

# 2nd panel - stages
par(mar=c(2, 1, 4.1, 4.1)) # margin adjustment
# plot
tsplot(stages, boxes="sys", shading="sys", ylim=c(0, 2), ylab="", xlim=4:95, 
       plot.args=list(axes=F, cex.lab=0.8), xlab="",labels.args=list(cex=0.8))
shades(stages$mid, as.matrix(extDatStg[,-1]), col="red") # background
for(i in 2:ncol(extDatStg)) 
  lines(stages$mid, extDatStg[ ,i] , lwd=0.5) # extinction - stages
mtext(line=1, text="stages", side=3, cex=1)

# originations
# 3rd panel - 10 myr bins
par(mar=c(5.1, 5.1, 1, 1)) # margin adjustment
# plot
tsplot(stages, boxes="sys", shading="sys", ylim=c(0, 3), ylab="Origination rates", 
       xlim=4:95, plot.args=list(axes=F, cex.lab=0.8),labels.args=list(cex=0.8), 
       xlab="Age (Ma)")
axis(1, cex.axis=0.8)
axis(2, cex.axis=0.8)
shades(tens$mid, as.matrix(oriDatTen[,-1]), col="darkgreen") # background
for(i in 2:ncol(oriDatTen)) 
  lines(tens$mid, oriDatTen[ ,i], lwd=0.5) # originations - BIN

# 4th panel - stages
par(mar=c(5.1, 1, 1, 4.1))# margin adjustment
# plot
tsplot(stages, boxes="sys", shading="sys", ylim=c(0, 2), ylab="", xlim=4:95, 
       plot.args=list(axes=F, cex.lab=0.8),labels.args=list(cex=0.8), xlab="Age (Ma)")
axis(1, cex.axis=0.8)
shades(stages$mid, as.matrix(oriDatStg[,-1]), col="darkgreen") # background
for(i in 2:ncol(oriDatStg)) 
  lines(stages$mid, oriDatStg[ ,i] , lwd=0.5) # originations - stages

# RGS: all code below this point was added to store the results from each loop into summary dataframes (and to add cross-validated loess models for all extinction rate metrics)

extDatTen.presum <- merge(extDatTen, tens, by="ten", all.x=T)
extDatTen.presum$no.sp <- as.numeric(no.sp)

extDatStg.presum <- merge(extDatStg, stages, by="stg", all.x=T)
extDatStg.presum$no.sp <- as.numeric(no.sp)

extDatTen.Sum <- rbind(extDatTen.Sum, extDatTen.presum)
extDatStg.Sum <- rbind(extDatStg.Sum, extDatStg.presum)

extModel.Ten.presum <- as.numeric()

# RGS add in <- cross-validated loess models for all extinction rate metrics!
for(i in 2:13){
  extModel.Ten.prepresum <- as.numeric()
  ages <- extDatTen.presum$mid[!is.na(extDatTen.presum[,i])]
  ext.rates <- as.numeric(na.omit(extDatTen.presum[,i]))
  extModel <- fANCOVA::loess.as(ages, ext.rates, degree = 1, 
                                criterion = "aicc", user.span = NULL, plot = FALSE, na.rm=T)
  predExt <- predict(extModel, newdata=data.frame(x=ages)) # extinctions
  extModel.Ten.prepresum$value <- predExt
  extModel.Ten.prepresum <- as.data.frame(extModel.Ten.prepresum)
  extModel.Ten.prepresum$mid <- ages
  extModel.Ten.prepresum$variable <- colnames(extDatTen.presum)[i]
  extModel.Ten.presum <- rbind(extModel.Ten.presum, extModel.Ten.prepresum)
}  

extModel.Ten.presum$no.sp <- as.numeric(no.sp)

extModel.Stg.presum <- as.numeric()

for(i in 2:13){
  extModel.Stg.prepresum <- as.numeric()
  ages <- extDatStg.presum$mid[!is.na(extDatStg.presum[,i])]
  ext.rates <- as.numeric(na.omit(extDatStg.presum[,i]))
  extModel <- fANCOVA::loess.as(ages, ext.rates, degree = 1, 
                                criterion = "aicc", user.span = NULL, plot = FALSE, na.rm=T)
  predExt <- predict(extModel, newdata=data.frame(x=ages)) # extinctions
  extModel.Stg.prepresum$value <- predExt
  extModel.Stg.prepresum <- as.data.frame(extModel.Stg.prepresum)
  extModel.Stg.prepresum$mid <- ages
  extModel.Stg.prepresum$variable <- colnames(extDatStg.presum)[i]
  extModel.Stg.presum <- rbind(extModel.Stg.presum, extModel.Stg.prepresum)
}  
  
extModel.Stg.presum$no.sp <- as.numeric(no.sp)

extModel.Ten.Sum <- rbind(extModel.Ten.Sum, extModel.Ten.presum)
extModel.Stg.Sum <- rbind(extModel.Stg.Sum, extModel.Stg.presum)

valBin.subset <- as.data.frame(valBin["extinctions",])
pBin.subset <- as.data.frame(pBin["extinctions",])

names(valBin.subset)[1] <- "correlation"
valBin.subset$variable <- rownames(valBin.subset)

names(pBin.subset)[1] <- "p-value"
pBin.subset$variable <- rownames(pBin.subset)

statsBin.subset <- merge(valBin.subset, pBin.subset, by="variable")
statsBin.subset$no.sp <- as.numeric(no.sp)

valStage.subset <- as.data.frame(valStage["extinctions",])
pStage.subset <- as.data.frame(pStage["extinctions",])

names(valStage.subset)[1] <- "correlation"
valStage.subset$variable <- rownames(valStage.subset)

names(pStage.subset)[1] <- "p-value"
pStage.subset$variable <- rownames(pStage.subset)

statsStage.subset <- merge(valStage.subset, pStage.subset, by="variable")
statsStage.subset$no.sp <- as.numeric(no.sp)

statsBin.sum <- rbind(statsBin.sum, statsBin.subset)
statsStage.sum <- rbind(statsStage.sum, statsStage.subset)

print(as.numeric(no.sp))
}

extDatTen.Sum$no.sp[is.na(extDatTen.Sum$no.sp)] <- "All"
extDatStg.Sum$no.sp[is.na(extDatStg.Sum$no.sp)] <- "All"
extModel.Ten.Sum$no.sp[is.na(extModel.Ten.Sum$no.sp)] <- "All"
extModel.Stg.Sum$no.sp[is.na(extModel.Stg.Sum$no.sp)] <- "All"
statsBin.sum$no.sp[is.na(statsBin.sum$no.sp)] <- "All"
statsStage.sum$no.sp[is.na(statsStage.sum$no.sp)] <- "All"

extDatTen.Sum$no.sp[extDatTen.Sum$no.sp == "4"] <- "4+"
extDatStg.Sum$no.sp[extDatStg.Sum$no.sp == "4"] <- "4+"
extModel.Ten.Sum$no.sp[extModel.Ten.Sum$no.sp == "4"] <- "4+"
extModel.Stg.Sum$no.sp[extModel.Stg.Sum$no.sp == "4"] <- "4+"
statsBin.sum$no.sp[statsBin.sum$no.sp == "4"] <- "4+"
statsStage.sum$no.sp[statsStage.sum$no.sp == "4"] <- "4+"

extDatTen.Sum$res <- "~10 Myrs"
extDatStg.Sum$res <- "Stages"
extModel.Ten.Sum$res <- "~10 Myrs"
extModel.Stg.Sum$res <- "Stages"
statsBin.sum$res <- "~10 Myrs"
statsStage.sum$res <- "Stages"

statsBin.sum$rich[grepl("raw",statsBin.sum$variable)] <- "raw"
statsBin.sum$rich[grepl("cr",statsBin.sum$variable)] <- "cr"
statsBin.sum$rich[grepl("sqs",statsBin.sum$variable)] <- "sqs"

statsBin.sum$rate[grepl("PC",statsBin.sum$variable)] <- "PC"
statsBin.sum$rate[grepl("C3t",statsBin.sum$variable)] <- "C3t"
statsBin.sum$rate[grepl("GF",statsBin.sum$variable)] <- "GF"
statsBin.sum$rate[grepl("2f3",statsBin.sum$variable)] <- "2f3"

statsBin.sum.clean <- statsBin.sum[,c(5, 6, 7, 4, 2, 3)]

statsStage.sum$rich[grepl("raw",statsStage.sum$variable)] <- "raw"
statsStage.sum$rich[grepl("cr",statsStage.sum$variable)] <- "cr"
statsStage.sum$rich[grepl("sqs",statsStage.sum$variable)] <- "sqs"

statsStage.sum$rate[grepl("PC",statsStage.sum$variable)] <- "PC"
statsStage.sum$rate[grepl("C3t",statsStage.sum$variable)] <- "C3t"
statsStage.sum$rate[grepl("GF",statsStage.sum$variable)] <- "GF"
statsStage.sum$rate[grepl("2f3",statsStage.sum$variable)] <- "2f3"

statsStage.sum.clean <- statsStage.sum[,c(5, 6, 7, 4, 2, 3)]

# save extinction rate summaries and statistics tables. 
# For Table S1 - statsBin.sum.clean and statsStage.sum.clean are simply transposed to the format illustrated in the paper. 
save(extDatTen.Sum, extDatStg.Sum, extModel.Ten.Sum, extModel.Stg.Sum, statsBin.sum.clean, statsStage.sum.clean, file="extinction.rates.ectotherms.20210630.RData")

extDatTen.Sum.subset <- extDatTen.Sum[,c(2:13, 19, 22, 23)]
extDatTen.Sum.long <- melt(extDatTen.Sum.subset, id.vars = c("mid", "no.sp", "res"))

extDatTen.Sum.long$rich[grepl("raw",extDatTen.Sum.long$variable)] <- "raw"
extDatTen.Sum.long$rich[grepl("cr",extDatTen.Sum.long$variable)] <- "cr"
extDatTen.Sum.long$rich[grepl("sqs",extDatTen.Sum.long$variable)] <- "sqs"

extDatTen.Sum.long$rate[grepl("PC",extDatTen.Sum.long$variable)] <- "PC"
extDatTen.Sum.long$rate[grepl("C3t",extDatTen.Sum.long$variable)] <- "C3t"
extDatTen.Sum.long$rate[grepl("GF",extDatTen.Sum.long$variable)] <- "GF"
extDatTen.Sum.long$rate[grepl("2f3",extDatTen.Sum.long$variable)] <- "2f3"


extDatStg.Sum.subset <- extDatStg.Sum[,c(2:13, 20, 26, 27)]
extDatStg.Sum.long <- melt(extDatStg.Sum.subset, id.vars = c("mid", "no.sp", "res"))

extDatStg.Sum.long$rich[grepl("raw",extDatStg.Sum.long$variable)] <- "raw"
extDatStg.Sum.long$rich[grepl("cr",extDatStg.Sum.long$variable)] <- "cr"
extDatStg.Sum.long$rich[grepl("sqs",extDatStg.Sum.long$variable)] <- "sqs"

extDatStg.Sum.long$rate[grepl("PC",extDatStg.Sum.long$variable)] <- "PC"
extDatStg.Sum.long$rate[grepl("C3t",extDatStg.Sum.long$variable)] <- "C3t"
extDatStg.Sum.long$rate[grepl("GF",extDatStg.Sum.long$variable)] <- "GF"
extDatStg.Sum.long$rate[grepl("2f3",extDatStg.Sum.long$variable)] <- "2f3"

extModelTen.Sum.long <- extModel.Ten.Sum # frame is already long, so just rename

extModelTen.Sum.long$rich[grepl("raw",extModelTen.Sum.long$variable)] <- "raw"
extModelTen.Sum.long$rich[grepl("cr",extModelTen.Sum.long$variable)] <- "cr"
extModelTen.Sum.long$rich[grepl("sqs",extModelTen.Sum.long$variable)] <- "sqs"

extModelTen.Sum.long$rate[grepl("PC",extModelTen.Sum.long$variable)] <- "PC"
extModelTen.Sum.long$rate[grepl("C3t",extModelTen.Sum.long$variable)] <- "C3t"
extModelTen.Sum.long$rate[grepl("GF",extModelTen.Sum.long$variable)] <- "GF"
extModelTen.Sum.long$rate[grepl("2f3",extModelTen.Sum.long$variable)] <- "2f3"

extModelStg.Sum.long <- extModel.Stg.Sum # frame is already long, so just rename

extModelStg.Sum.long$rich[grepl("raw",extModelStg.Sum.long$variable)] <- "raw"
extModelStg.Sum.long$rich[grepl("cr",extModelStg.Sum.long$variable)] <- "cr"
extModelStg.Sum.long$rich[grepl("sqs",extModelStg.Sum.long$variable)] <- "sqs"

extModelStg.Sum.long$rate[grepl("PC",extModelStg.Sum.long$variable)] <- "PC"
extModelStg.Sum.long$rate[grepl("C3t",extModelStg.Sum.long$variable)] <- "C3t"
extModelStg.Sum.long$rate[grepl("GF",extModelStg.Sum.long$variable)] <- "GF"
extModelStg.Sum.long$rate[grepl("2f3",extModelStg.Sum.long$variable)] <- "2f3"

extDatAll.Sum.long <- rbind(extDatStg.Sum.long, 
                            extDatTen.Sum.long)

extModelAll.Sum.long <- rbind(extModelStg.Sum.long, 
                              extModelTen.Sum.long)

# Reload deeptime for plotting

library(deeptime)
adapted.periods <- deeptime::periods
adapted.periods$abbr[1:2] <- c("","N")

# Generate Figure S1
extDatAll.Sum.long$no.sp <- factor(extDatAll.Sum.long$no.sp, levels = c("All", "1", "2", "3", "4+"))

ext.sum <- ggplot(extDatAll.Sum.long, aes(x=mid, y=value))+
  geom_line(aes(group = variable, color = rich, linetype=rate), alpha=0.75, size=.5)+
  coord_geo(xlim=c(541,-1), ylim=c(-0.1,2.65),expand=FALSE, # Geologic timescale added for clarity
            pos = as.list(rep("bottom", 1.3)),
            abbrv=list( T),
            dat = list(adapted.periods),
            height = list(unit(1, "lines")),
            size=list(4),
            bord=list(c("left", "bottom", "right")), lwd=as.list(.5))+
  theme_minimal()+scale_x_reverse()+
  scale_color_manual(name = "Richness Metric", values=c("#F1AE43", "#5B988C", "#38496D"), labels=c( "Classical Rarefaction", "Raw Occurrences", "Shareholder Quorum\nSubsampling"))+ 
  scale_linetype_manual(name = "Extinction Rate Metric", values=c("solid", "longdash", "twodash", "dotdash"), labels=c( "Second-for-third\nsubstitution", "Corrected\nthree-timer", "Gap-filler", "Per\ncapita"))+
  ylab("Extinction Rate")+xlab("Time (Ma)")+
  theme(panel.border = element_rect(fill=NA,color="black", size=1,linetype="solid"),
        axis.ticks = element_line(size=.5), 
        axis.title = element_text(size=20),
        axis.text =  element_text(size=16, colour="black"),
        legend.title = element_text(size=16),
        legend.text =  element_text(size=12),
        strip.text =  element_text(size=16),
        plot.margin = margin(10,30,10,10),
        legend.position="top",legend.box = "vertical",
        legend.key.width = unit(1.5,"cm"),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
  facet_grid(no.sp ~ res)+
  guides(color = guide_legend(override.aes = list(size = 1)),
         linetype = guide_legend(override.aes = list(size = 1)))

# Save as Figure S1
ggsave("Extinction rates  sum ectotherms (Figure S1).pdf", ext.sum, width=9, height=11)

# Generate Figure S2
extModelAll.Sum.long$no.sp <- factor(extModelAll.Sum.long$no.sp, levels = c("All", "1", "2", "3", "4+"))

ext.model.sum <- ggplot(extModelAll.Sum.long, aes(x=mid, y=value))+
  geom_line(aes(group = variable, color = rich, linetype=rate), alpha=0.85, size=.5)+
  coord_geo(xlim=c(541,-1), ylim=c(-0.1,1.7),expand=FALSE, # Geologic timescale added for clarity
            pos = as.list(rep("bottom", 1.3)),
            abbrv=list( T),
            dat = list(adapted.periods),
            height = list(unit(1, "lines")),
            size=list(4),
            bord=list(c("left", "bottom", "right")), lwd=as.list(.5))+
  theme_minimal()+scale_x_reverse()+
  scale_color_manual(name = "Richness Metric", values=c("#F1AE43", "#5B988C", "#38496D"), labels=c( "Classical Rarefaction", "Raw Occurrences", "Shareholder Quorum\nSubsampling"))+ 
  scale_linetype_manual(name = "Extinction Rate Metric", values=c("solid", "longdash", "twodash", "dotdash"), labels=c( "Second-for-third\nsubstitution", "Corrected\nthree-timer", "Gap-filler", "Per\ncapita"))+
  ylab("Extinction Rate")+xlab("Time (Ma)")+
  theme(panel.border = element_rect(fill=NA,color="black", size=1,linetype="solid"),
        axis.ticks = element_line(size=.5), 
        axis.title = element_text(size=20),
        axis.text =  element_text(size=16, colour="black"),
        legend.title = element_text(size=16),
        legend.text =  element_text(size=12),
        strip.text =  element_text(size=16),
        plot.margin = margin(10,30,10,10),
        legend.position="top",legend.box = "vertical",
        legend.key.width = unit(1.5,"cm"),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
  facet_grid(no.sp ~ res)+
  guides(color = guide_legend(override.aes = list(size = 1)),
         linetype = guide_legend(override.aes = list(size = 1)))

# Save as Figure S2
ggsave("Extinction rates model sum ectotherms (Figure S2).pdf", ext.model.sum, width=9, height=11)

# Generate Figure 1a
sum.extStg.ecto <- extDatStg.Sum.long %>%
  filter(no.sp == "All") %>%
  group_by(mid) %>%
  summarize(mean = mean(value, na.rm = T), min = min(value, na.rm = T), max = max(value, na.rm = T))

sum.extStg.ecto$big.five <- "N"
sum.extStg.ecto$big.five[sum.extStg.ecto$mid == 449.10000] <- "Y"
sum.extStg.ecto$big.five[sum.extStg.ecto$mid == 365.55000] <- "Y"
sum.extStg.ecto$big.five[sum.extStg.ecto$mid == 253.17000] <- "Y"
sum.extStg.ecto$big.five[sum.extStg.ecto$mid == 204.90000] <- "Y"
sum.extStg.ecto$big.five[sum.extStg.ecto$mid == 69.05000] <- "Y"


ext.sum <- ggplot(sum.extStg.ecto, aes(x=mid, ymin=min, ymax=max, y=mean))+
  geom_ribbon(alpha=.5,linetype=2, fill="grey80",color=NA )+
  geom_line(linetype=1, alpha=1, size=.5)+
  geom_point(shape=21, alpha=1, size=2.5, aes(fill=big.five))+
  scale_fill_manual(values = c("goldenrod", "grey90"), 
                    labels = c("Background", "'Big Five'"), 
                    name = "Extinction")+
  coord_geo(xlim=c(541,-1), ylim=c(-0.1,1.3),expand=FALSE, # Geologic timescale added for clarity
            pos = as.list(rep("bottom", 1)),
            abbrv=list( T),
            dat = list(adapted.periods),
            height = list(unit(2, "lines")),
            size=list(7),
            bord=list(c("left", "bottom", "right")), lwd=as.list(1))+
  theme_minimal()+scale_x_reverse()+
  ylab("Extinction Rate")+xlab("Time (Ma)")+
  theme(panel.border = element_rect(fill=NA,color="black", size=2,linetype="solid"),
        axis.ticks = element_line(size=1.12), 
        axis.title = element_text(size=26),
        axis.text =  element_text( size=22, colour="black"),
        plot.margin = margin(10,30,10,10),
        legend.text = element_text( size=22, colour="black"),
        legend.justification=c(1,1),legend.position=c(0.98, 0.98),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        panel.grid.major = element_blank(),panel.grid.minor = element_blank())

# Save as Figure 1a
ggsave("Extinction rate summary stages (Figure 1a).pdf", ext.sum, width=9.7, height=5.5)

